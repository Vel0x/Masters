\chapter{Prediction Evaluation}\label{prediction_evaluation}

	\section{Motivation}\label{prediction_evaluation_motivation}

		In sampling data it is almost inevitable that information will be missed. In order to accurately reconstruct the original data we must take certain precautions. One of these precautions is making sure that we sample at a rate high enough to reconstruct all information. This rate goes by the term \emph{Nyquist rate}~\cite{nyquistrate}. The Nyquist rate is the rate of taking measurements such that the original information can be reconstructed without aliasing occurring. This essentially means that we need to have a sample rate of twice the rate of the smallest changes in a signal. If we do not have this sampling rate then it is possible for us to lose information during the sampling process. 

		\centerimagewide{./images/Nyquist_Example.png}{Here we can see an example of a sample rate on the function $y = sin(4x)$ being insufficient. This sample rate of 1.2 is insufficient for the graph since it's period is $\frac{2\pi}{4} \equiv 1.570$. A sample rate of $\frac{1.570}{2} \equiv 0.785$ is required to be sufficient.}{fig:nyquist_example}

		When measuring air quality we almost certainly will not meet the Nyquist rate and therefore need to compensate. Without any interpolation or extrapolation methods the best we can do is generate data similar to that seen in figure~\ref{fig:stationarypollutantbuildup}. In this figure we see that we only have information where pollutants were measured. In this case it was done with a sensor on a bus and therefore limited to the roads. 

		\centerimagewide{./images/StationaryPollutantBuildUp.png}{A map of air pollution around Edinburgh city centre. Image uses \emph{Open Street Map}.}{fig:stationarypollutantbuildup}

		\centerimage{.5\textwidth}{./images/zurichpm.jpg}{A heatmap of particulate matter in Zurich. The higher pollution levels can clearly be seen to follow transport infrastructure.~\cite{opensensezurich}}{fig:zurichheatmap}

		The solution is to interpolate and extrapolate our data so that what we see in figure~\ref{fig:stationarypollutantbuildup} can be converted into something similar to figure~\ref{fig:zurichheatmap} where we have estimations of pollutant levels in areas where we did not take measurements. 

		With air quality data many institutes and organisations use complex methods of interpolating the data such as \emph{Land Use Regression (LUR)}~\cite{lurtraffic}. LUR and other advanced models~\cite{reviewofaqmodels} have advantages over simple interpolation models in that they can take other information into account. By feeding in information about sources of pollution, terrain, buildings, etc. we can greatly improve the estimations of these models. However, the disadvantage of these models is the complexity. As such it would be useful to determine if simple interpolation algorithms, such as those designed for image or sound processing, are effective at interpolation air quality data sets. The algorithms which will be tested are those discussed in section~\ref{background_interpolation_methods}. These algorithms are: 

		\begin{itemize}
	        \item Bilinear interpolation
	        \item Bicubic interpolation
	        \item Natural neighbour interpolation
	        \item Barnes interpolation
	        \item Nearest neighbour interpolation
	        \item Inverse distance weighting interpolation
	    \end{itemize}

	    The bilinear and bicubic algorithms are generally used for interpolating pixels in images when resized. Natural neighbour, nearest neighbour and inverse distance weighted are more generic interpolation algorithms, and Barnes interpolation has seen application in weather forecasting~\cite{barnesinterpolation}. 



	\section{Methodology}\label{prediction_evaluation_methodology}

		In order to evaluate the efficiency of these algorithms we will employ a simple approach. This approach will use a data set with complimentary values. We will run the data set through the interpolation algorithms and compare the interpolated values to known values from the complimentary data set. The difference between these values will provide a metric as to the accuracy of our interpolation algorithms. 

		\subsection{Data Sets}\label{prediction_evaluation_methodology_data_sets}

			\subsubsection{OpenSense}\label{prediction_evaluation_methodology_data_sets_opensense}
				The first data set, provided by the \emph{OpenSense} group at \emph{ETH Zurich}, provides ozone concentrations based on measurements taken from trams moving around the city of Zurich, Switzerland. This data set only has measurements along the tram tracks. We will perform our interpolation on this data set giving a two dimensional map covering the city of estimated ozone concentrations. We will then use information taken from static sensors, which are not along tram tracks, to measure the accuracy of these interpolation algorithms. 

				\centerimagewide{./images/Zurich_Ozone.png}{A scatter plot of the Zurich data points. The hue of the colour of the points is directly proportional to the Ozone concentration.}{fig:zurich_ozone} \todo{Reference this somewhere?}

				This data set poses certain problems. The main issue is that we only have two static sensors which we can use to determine the accuracy of our algorithms. Ideally we would have many more. Two sensors is enough to show whether some algorithms perform better than others, but only for those points. Their usefulness across an entire city should be reviewed carefully in further experimentation with a more comprehensive data set. A second issue arises from the temporal changes in the data. If we were to take the measurements at any given point in time we would only have as many data points as there are sensors. By selecting a temporal window of measurements we can increase the number of readings we have, which are at different locations due to the motion of the trams, and as such give a denser set of data points to the algorithms, which will allow for more accurate calculations. In taking this temporal window we risk missing out on some information. Should a pollutant change rapidly in the space of a single window, this information would be lost, and may indeed skew our results. Experimentation revealed that a window of 30 minutes is enough to cover as much area as possible, with minimal overlap of tram routes. 


			\subsubsection{Air Quality Scotland}\label{prediction_evaluation_methodology_data_sets_air_quality_scotland}

				The data set from \emph{Air Quality Scotland} is much simpler, and therefore less useful, than the OpenSense data set. This data set consists of eight static air quality monitoring stations across Edinburgh. Out of these eight stations, five form the convex hull of the data set, and one of them is in a high traffic zone. The result of this is that we only have three points which we can use to check our calculations, and one of these is going to have a large discrepancy between the actual values and the estimated values. Furthermore, this data set only has the means at one hour intervals and so the temporal drift mentioned in the above section still applies. However, one advantage that this data set does provide is three different chemical readings to use. 

		\subsection{Data Structure}\label{prediction_evaluation_methodology_data_structure}
			
			In order to use these data sets in the various algorithms we must change the data structure slightly. Two of the algorithms, bilinear interpolation, and bilinear interpolation, work when the data is on a regular grid, which these data sets are not. As such we need to map them onto a regular grid. For the algorithms which are not limited to a grid we do not have to

			With the Zurich data set the area which we are taking measurements from is roughly $5km$ by $5km$. By setting a resolution of 200 by 200 for the grid we can calculate the interpolated values in regions roughly $25m$ by $25m$. Using the same resolution on the Edinburgh data we move our data points from a map of size $15km$ by $9km$ to a grid with cells of size $75m$ by $45m$. 


		

    \section{Results}\label{prediction_evaluation_results}

        \subsection{Nearest Neighbour}\label{prediction_evaluation_results_nearest_neighbour}


        \centerimagewide{./images/Nearest_Neighbour_Zurich.png}{The result of applying the nearest neighbour algorithm to the data seen in figure~\ref{fig:zurich_ozone}.}{fig:nearest_neighbour_zurich}

        When we apply the nearest neighbour algorithm to the data set seen in figure~\ref{fig:zurich_ozone} we get a result similar to that seen in figure~\ref{fig:nearest_neighbour_zurich}. When we apply it to the Edinburgh data set we get results seen in table~\ref{tab:nearest_neighbour_difference_results} and in figure~\ref{fig:nearest_neighbour_difference_results}. 

        \begin{table}[H]
        	\centering
    		\begin{tabular}{|l|l|l|l|l|l|l|}
    			\hline
		        Pollutant & Time & Actual & Estimated & Difference & Absolute Difference & Difference (\%) \\ \hline
				NO & 1 & 19.00 & 20.67 & 1.67 & 1.67 & 8.77 \\
				NO & 2 & 10.67 & 10.00 & -0.67 & 0.67 & 6.25 \\
				NO & 3 & 26.00 & 12.33 & -13.67 & 13.67 & 52.56 \\
				NO & 4 & 17.00 & 12.33 & -4.67 & 4.67 & 27.45 \\
				NO & 5 & 33.00 & 20.67 & -12.33 & 12.33 & 37.37 \\
				NO & 6 & 42.00 & 37.00 & -5.00 & 5.00 & 11.90 \\
				NO & 7 & 138.00 & 97.67 & -40.33 & 40.33 & 29.23 \\
				NO & 8 & 226.00 & 186.67 & -39.33 & 39.33 & 17.40 \\
				NO$_{2}$ & 1 & 40.67 & 41.33 & 0.67 & 0.67 & 1.64 \\
				NO$_{2}$ & 2 & 39.00 & 40.67 & 1.67 & 1.67 & 4.27 \\
				NO$_{2}$ & 3 & 42.00 & 42.67 & 0.67 & 0.67 & 1.59 \\
				NO$_{2}$ & 4 & 42.00 & 41.00 & -1.00 & 1.00 & 2.38 \\
				NO$_{2}$ & 5 & 42.33 & 39.00 & -3.33 & 3.33 & 7.87 \\
				NO$_{2}$ & 6 & 44.67 & 45.33 & 0.67 & 0.67 & 1.49 \\
				NO$_{2}$ & 7 & 63.00 & 66.33 & 3.33 & 3.33 & 5.29 \\
				NO$_{2}$ & 8 & 85.67 & 90.33 & 4.67 & 4.67 & 5.45 \\
				NO$_{X}$ & 1 & 70.00 & 73.33 & 3.33 & 3.33 & 4.76 \\
				NO$_{X}$ & 2 & 55.67 & 56.33 & 0.67 & 0.67 & 1.20 \\
				NO$_{X}$ & 3 & 82.33 & 61.67 & -20.67 & 20.67 & 25.10 \\
				NO$_{X}$ & 4 & 68.33 & 60.00 & -8.33 & 8.33 & 12.20 \\
				NO$_{X}$ & 5 & 93.00 & 70.67 & -22.33 & 22.33 & 24.01 \\
				NO$_{X}$ & 6 & 108.67 & 101.67 & -7.00 & 7.00 & 6.44 \\
				NO$_{X}$ & 7 & 274.67 & 216.00 & -58.67 & 58.67 & 21.36 \\
				NO$_{X}$ & 8 & 431.67 & 376.33 & -55.33 & 55.33 & 12.82 \\ \hline
			\end{tabular}
			\caption{The results of running the nearest neighbour algorithm, averaged across three different measurement points.}
			\label{tab:nearest_neighbour_difference_results}
		\end{table}

		\centerimagewide{./images/Nearest_Neighbour_Percentage_Differences.png}{The results of running the nearest neighbour algorithm as a scatter plot, averaged across three different measurement points.}{fig:nearest_neighbour_difference_results}

		However, the most important information we can glean from this, the information which we will use for our comparison, is the mean across all time periods. That data is shown in table~\ref{tab:nearest_neighbour_difference_results_mean}

		\begin{table}[H]
			\centering
    		\begin{tabular}{|l|l|l|l|l|l|l|}
    			\hline
		        Pollutant & Difference (\%) \\ \hline
				NO & 23.87 \\
				NO$_{2}$ & 3.75 \\
				NO$_{X}$ & 13.49 \\ \hline
			\end{tabular}
			\caption{The mean results of table~\ref{tab:nearest_neighbour_difference_results}.}
			\label{tab:nearest_neighbour_difference_results_mean}
		\end{table}

        \subsection{Inverse Distance Weighting}\label{prediction_evaluation_results_inverse_distance_weighting}

        \subsection{Bilinear}\label{prediction_evaluation_results_bilinear}

        \subsection{Bicubic}\label{prediction_evaluation_results_bicubic}

        \subsection{Natural Neighbour}\label{prediction_evaluation_results_natural_neighbour}

        \subsection{Barnes}\label{prediction_evaluation_results_barnes}

        	Barnes interpolation, rather than just being evaluated for suitability, also has parameters which need to be tuned. In order to do this multiple experiments need to be run in which we vary these parameters for data sets in order to determine which parameters are optimal. These parameters are the number of error correcting passes we make over the data, and the distance constant which we use to weight the values we use to interpolate. 

        	In order to determine the optimal number of passes for this algorithm we used the datasets. By running the algorithm with a varying number of passes, but with the same distance constant, we can see how these subsequent passes alter the results. Figure~\ref{fig:barnes_no_passes_results} shows a scatter plot of the absolute percentage difference between the estimated value and the actual value. These results were calculated for all 3 data points in the Edinburgh data set which can be used and the mean taken. We plot the results for each different pass in the range of zero through to twenty-four. 

        	\centerimagewide{./images/Barnes_Mean_Difference_NO.png}{A scatter plot showing the difference for 25 different passes for each data time frame. The distance constant is 1024.}{fig:barnes_no_passes_results}

        	As we can see, with no error correcting passes, we have a reasonable approximation of the result. The first subsequent pass corrects this error, however all subsequent passes do very little to improve the results. In the interest of computation time it would be reasonable to limit the number of passes to no more than five. 

        	We now need to consider how the error field of the Barnes interpolation algorithm changes with respect to the distance factor. In order to achieve this a similar test to the above was run, with the number of passes remaining constant and the distance factor changing. 

        	\centerimagewide{./images/Barnes_Distance_Factor_NO.png}{A scatter plot showing the error field difference for the ten different distance factors.}{fig:barnes_distance_factor_results}

        	Figure~\ref{fig:barnes_distance_factor_results} shows us these effects. As we can see, the greater the distance factor, the greater the difference successive passes make. This appears to continue until a distance of around $2^{17}$. At this point the differences start to decrease again. Furthermore we can see that the difference between passes decreases as the number of passes increases. Due to computational time requirements these results were only calculated to 24 passes. However, increasing the runtime by 5 more passes, a run time increase of 20\% would only yield a potential accuracy increase of around a one hundredth of one percent. 

        	\tdi{Continue this. Get the results.}
        

        \subsection{Conclusion}

        	\tdi{Remember, instead of just stating which is better, we need to know if it is good enough}

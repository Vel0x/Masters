\chapter{Data Gathering Performance}\label{data_gathering_performance} 

    \todo{This title needs to reflect the first goal of the project}

    The first and foremost goal of this project is to study the performance of an urban mobile air quality monitoring system which leverages public transport vehicles and the effects of various parameters. In order to do this we must decide on how we will measure the performance as well as what parameters we will vary. 

    In order to measure the performance of the system, we must pay attention to the use of such a system. With air quality sensors on buses our priorities are receiving as much data as possible and receiving said data with minimal latency. These priorities define our metrics for us. Our metrics shall be the percentage of readings which are returned to the server, and the time which it takes for this to happen. In doing this we must consider how the effects of varying different parameters change the results. The parameters which we will vary are the storage space on the device, that is to say how many packets we can store in the queue at any one time, the number of access points scattered around the city, the power levels of the transmitter and receiver and finally the utility of using a priority queue instead of a regular queue when using opportunistic forwarding.
	

    \section{Queue Size}\label{data_gathering_performance_queue_size}

        \tdi{Reword the end of this.}

        Varying the size of the queue was a simple change but one which would yield great insight. The queue size is how many packets or readings we can store on the sensor at any one time. When creating a physical implementation of this model one of the prohibiting factors is cost, and the storage size is almost directly correlated with the cost of the device. As such it is extremely important to evaluate the effectiveness of the model when using a smaller queue size. In order to test this, we ran the simulation with all 145 buses and 50 access points but a variable queue size. The sizes of queue checked were 10, 30, 100 and 2000. The results of running this simulation, a can be seen in table~\ref{tab:queue_size} and figure~\ref{fig:queue_size}. 

        From the data it is clear that having a larger queue size has a positive effect on the percentage of packets which are delivered. In fact, we can see that we more than triple the packet delivery rate by increasing the queue size from 10 packets to 2000. Tripling the queue size from 10 until 30 has a relatively small performance increase. Due to this, it is theorised that even if the queue were non existent we would not decrement our percentage of packets received by a significant amount. The reason for this is that 14\% is likely to be close to the mean amount of time that a bus is in contact with an access point for. When a bus is in contact with an access point it can send the packets immediately without losing them. With a buffer size of 10, we can only store packets for 10 seconds before contacting an access point. If the mean amount of time a bus is in contact with an access point is 10 seconds, then removing this queue size would halve the packet delivery rate. From this we can see that the smaller the queue size, the less effect it has on the packet delivery percentage. However, following the trend from this information, it would require a queue size of between 150,000 and 200,000 in order to get a 100\% packet delivery rate. In terms of storage space this is between 3.6MB and 4.8MB. However, 150,000 readings is just under 42 hours worth of data. We can expect an upload of at least every 24 hours or 86,400 seconds and so there is no need for the size to be greater than this, nor can we trust the results. 

        As the queue size increases we increase the ratio of packets delivered but also increase the latency. This is an expected behaviour due to the fact that more packets will be stored and therefore they have an effect on the latency. 

        \begin{landscape}
            \begin{table}
                \centering
                \begin{tabularx}{\linewidth}{|X|X|X|X|X|X|X|X|}
                    \hline
                    \multicolumn{1}{|X|}{\centering Queue \\ Size} & 
                    \multicolumn{1}{|X|}{\centering Number \\ of APs} & 
                    \multicolumn{1}{|X|}{\centering Mean \\ Latency (s)} & 
                    \multicolumn{1}{|X|}{\centering Standard \\ Deviation (s)} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Sent} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Received} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Dropped} & 
                    \multicolumn{1}{|X|}{\centering \% \\ Received} \\
                    \hline
                    10 & 50 & 0.485136283 & 0.482648346 & 2726636.5 & 207879.1667 & 1049162.833 & 14.34 \\
                    30 & 50 & 3.075895053 & 3.067765 & 2757366.667 & 238289.5 & 1020537.167 & 16.43 \\
                    100 & 50 & 16.72300115 & 16.72759292 & 2826758 & 307587.5 & 955076.8333 & 21.21 \\
                    2000 & 50 & 419.8375185 & 420.5738133 & 3220432.833 & 700218.1667 & 537348.1667 & 48.29 \\
                    \hline
                \end{tabularx}
                \caption{The results of changing the number of access points.}
                \label{tab:num_aps}
            \end{table}
        \end{landscape}

        \tdi{Insert chart of this data here}

    \section{Number of Access Points}\label{data_gathering_performance_number_of_access_points}

        The interest in varying the number of access points (APs) stems from the fact that this is a variable which is outwith our control. By setting a performance target we can find the minimum number of access points we would need to meet this target. This will show us if our system could potentially work in a situation with less APs (in reality there are many more). The results of varying the number of APs under different queue sizes is shown in table~\ref{tab:num_aps} and figure~\ref{fig:num_aps}.

        \begin{landscape}
            \begin{table}
                \centering
                \begin{tabularx}{\linewidth}{|X|X|X|X|X|X|X|X|}
                    \hline
                    \multicolumn{1}{|X|}{\centering Queue \\ Size} & 
                    \multicolumn{1}{|X|}{\centering Number \\ of APs} & 
                    \multicolumn{1}{|X|}{\centering Mean \\ Latency (s)} & 
                    \multicolumn{1}{|X|}{\centering Standard \\ Deviation (s)} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Sent} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Received} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Dropped} & 
                    \multicolumn{1}{|X|}{\centering \% \\ Received} \\
                    \hline
                    10 & 10 & 0.641546601 & 0.628416482 & 2837154.833 & 77350 & 1166151.5 & 5.33 \\
                    10 & 20 & 0.6377266 & 0.629448882 & 2806916.5 & 116009.5 & 1132689.333 & 8.00 \\
                    10 & 30 & 0.591328457 & 0.581260675 & 2775618.5 & 153667.3333 & 1098895.167 & 10.60 \\
                    10 & 40 & 0.557965006 & 0.552124348 & 2754254.5 & 179980.5 & 1076107.833 & 12.41 \\
                    10 & 50 & 0.485136283 & 0.482648346 & 2726636.5 & 207879.1667 & 1049162.833 & 14.34 \\
                    30 & 10 & 4.016328768 & 3.955641293 & 2851801 & 91969.33333 & 1151698.333 & 6.34 \\
                    30 & 20 & 3.978558616 & 3.946152299 & 2829299.333 & 138289.1667 & 1111176 & 9.54 \\
                    30 & 30 & 3.70040839 & 3.653918235 & 2802597.667 & 180843 & 1072978 & 12.47 \\
                    30 & 40 & 3.468226904 & 3.445352797 & 2783234.5 & 209719.8333 & 1047929 & 14.46 \\
                    30 & 50 & 3.075895053 & 3.067765 & 2757366.667 & 238289.5 & 1020537.167 & 16.43 \\
                    100 & 10 & 25.04383636 & 24.87469435 & 2898691.5 & 138808.1667 & 1105236.667 & 9.57 \\
                    100 & 20 & 23.43013378 & 23.36990619 & 2893541.833 & 202327.5 & 1049191 & 13.95 \\
                    100 & 30 & 20.75840795 & 20.63997577 & 2872975.833 & 250772.3333 & 1005479.667 & 17.29 \\
                    100 & 40 & 19.17159602 & 19.16736037 & 2856199.833 & 282431.8333 & 978600.3333 & 19.48 \\
                    100 & 50 & 16.72300115 & 16.72759292 & 2826758 & 307587.5 & 955076.8333 & 21.21 \\
                    2000 & 10 & 617.8010824 & 617.0399697 & 3369388.667 & 608908.8333 & 614393.3333 & 41.99 \\
                    2000 & 20 & 518.9917672 & 519.4679392 & 3351993.5 & 660129.8333 & 570220.6667 & 45.53 \\
                    2000 & 30 & 468.1774747 & 468.9056058 & 3302144.333 & 679437.8333 & 554234.8333 & 46.86 \\
                    2000 & 40 & 436.3411623 & 436.5999819 & 3271823.667 & 697031.5 & 539818.6667 & 48.07 \\
                    2000 & 50 & 419.8375185 & 420.5738133 & 3220432.833 & 700218.1667 & 537348.1667 & 48.29 \\
                    \hline
                \end{tabularx}
                \caption{The results of changing the number of access points.}
                \label{tab:num_aps}
            \end{table}
        \end{landscape}

        \tdi{Insert chart of this}

        From this data we can see that the greater the number of access points the lower the latency and the greater the packet delivery rate. When adding a larger queue size the packet delivery rate increases but there is no improvement in latency, which manifests as a latency increase overall. With increasing the number of APs that disadvantage is no longer present. At smaller queue sizes we can double or even triple the packet delivery rate with more access points. It should be noted that as discussed in chapter~\ref{simulation} simulation time was one of the main factors for limiting the simulations to at most 50 access points. With the dataset we had over 2,000 access points. As such we can expect a much greater packet delivery rate with a significantly lower mean latency in an actual realisation of this model. 


    \section{Transmitter and Receiver Power}\label{data_gathering_performance_transmitter_and_reciever_power}

        A simple way of increasing the access point coverage of the city is to increase the power levels of the access points and receivers. This increase in power level causes them to serve a greater area without the need for any more access points. European legislation restricts all 2.4GHz radios to an equivalent isotropically radiated power level of 100mW~\cite{rackley2011wireless}. Converting to dBm gives a value of 19.93dBm, or approximately 20dBm. In practice we cannot increase the power levels of the radios without breaking the law, however we can do this with a simulator. In order to determine the effect that power level has on the data collection performance we tested the model at 5 different power levels. These power levels were 18, 19, 20, 21 and 22 dBm, which corresponds to 64.00, 80.63, 101.59, 128.00 and 161.27 respectively. Furthermore, we also must acknowledge that this is in fact a simulation and therefore not identical to a physical implementation. As such we need to also acknowledge that the propagation model will not be perfect. In order to find the most realistic model for our simulation we tried using 2 different models for this test. These propagation models were the Rayleigh model and the Log Normal Shadowing model. The results of this test, using a queue size of 2000 and all 50 access points, are shown in table~\ref{tab:power_level} and figure~\ref{fig:power_level}. 

        \begin{landscape}
            \begin{table}
                \centering
                \begin{tabularx}{\linewidth}{|X|X|X|X|X|X|X|X|X|}
                    \hline
                    \multicolumn{1}{|X|}{\centering Power} & 
                    \multicolumn{1}{|X|}{\centering Model} & 
                    \multicolumn{1}{|X|}{\centering Mean \\ Latency (s)} & 
                    \multicolumn{1}{|X|}{\centering Standard \\ Deviation (s)} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Generated} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Sent} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Received} & 
                    \multicolumn{1}{|X|}{\centering Packets \\ Dropped} & 
                    \multicolumn{1}{|X|}{\centering \% \\ Received} \\
                    \hline
                    18 & Rayleigh & 154.12 & 323.82 & 294,743.60 & 522,092.80 & 238,455.20 & 1,986.40 & 80.74 \\
                    18 & Log Normal Shadowing & 122.62 & 278.35 & 279,041.50 & 452,601.00 & 225,997.50 & 2,268.75 & 79.72 \\
                    19 & Rayleigh & 128.94 & 281.37 & 270,759.00 & 473,615.20 & 213,552.20 & 933.20 & 78.72 \\
                    19 & Log Normal Shadowing & 169.79 & 363.87 & 527,232.75 & 868,566.75 & 431,838.25 & 34,279.50 & 83.15 \\
                    20 & Rayleigh & 150.13 & 322.72 & 299,218.50 & 525,681.25 & 240,421.75 & 1,445.50 & 80.26 \\
                    20 & Log Normal Shadowing & 150.72 & 316.18 & 470,629.50 & 782,219.50 & 388,500.00 & 20,491.00 & 81.03 \\
                    21 & Rayleigh & 116.22 & 255.51 & 220,911.00 & 383,434.50 & 169,469.00 & 0.00 & 76.79 \\
                    21 & Log Normal Shadowing & 86.43 & 203.14 & 179,300.00 & 290,371.00 & 134,968.00 & 0.00 & 75.27 \\
                    22 & Rayleigh & 135.75 & 294.37 & 270,957.00 & 471,525.67 & 218,749.33 & 977.33 & 80.54 \\
                    22 & Log Normal Shadowing & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
                    \hline
                \end{tabularx}
                \caption{The results of changing the power level of the radios.}
                \label{tab:power_level}
            \end{table}
        \end{landscape}

        In terms of the propagation model a significant problem was found which may be apparent from the final row in table~\ref{tab:power_level}. There appeared to be a bug in the implementation in Omnet++ in which the simulation tended to crash when used with this simulation after some indeterminable period of time. Generally we did not get a simulation time of longer than 2,000 seconds which is significantly shorter than the 10,000 seconds we had been using previously. In some cases we did not get readings for particular runs using the Log Normal Shadowing model and therefore the mean result was made up of just 3 or 4 readings instead of 5. Furthermore, the effect that the software bugs had on the simulation when also trying the Log Normal Shadowing model also seemed to affect the simulation when going using the Rayleigh model, which is the model used for all other experiments. As such no further experiments were performed on propagation models. 

        With the results we have we can still analyse the effect that power level has. The effect is almost non-existent. We would expect to see a decrease in latency and increase in packet delivery as the power level goes up, however we saw no such increase. It is theorised that the reason for this is that the packet upload time is dominated by the bandwidth of the connection. By increasing the power levels we may be in contact with an access point for an extra few seconds, however as we can upload 2000 packets in less than a second, which is an effective rate of 48Kbps, this extra time only limits the delivery time of a single packet or two. Over a greater period of time, should the simulation have not crashed, it is possible that we may have seen a very small correlation, however it is insignificant at this time. 


    \section{Opportunistic Forwarding}\label{data_gathering_performance_opportunistic_forwarding}

        Opportunistic forwarding is the most significant change to the model that we will attempt. The implementation details are discussed in section~\ref{simulation_opportunistic_forwarding}. With the implementation discussed, the only way to evaluate the performance is to run a simulation both with and without opportunistic forwarding. The results of this are shown in table~\ref{tab:opportunistic_forwarding} and figure~\ref{fig:opportunistic_forwarding}.

        \begin{landscape}
            \begin{table}
                \centering
                \begin{tabularx}{\linewidth}{|X|X|X|X|X|X|X|X|X|X|X|}
                    \hline
                    \multicolumn{1}{|X|}{\centering Queue Size} & \multicolumn{1}{|X|}{\centering Mean Latency (s)} & \multicolumn{1}{|X|}{\centering Standard Deviation} & \multicolumn{1}{|X|}{\centering AP Received} & \multicolumn{1}{|X|}{\centering AP Unique} & \multicolumn{1}{|X|}{\centering Packets Sent} & \multicolumn{1}{|X|}{\centering Packets Successful} & \multicolumn{1}{|X|}{\centering Packets Stored} & \multicolumn{1}{|X|}{\centering Packets Dropped} \\
                    \hline
                    10 & 0.23 & 1.39 & 463,908 & 421,622 & 2,160,545 & 463,239 & 57,673 & 833,967 \\
                    30 & 1.52 & 5.96 & 626,482 & 499,921 & 2,200,064 & 598,384 & 128,859 & 759,144 \\
                    100 & 12.02 & 30.38 & 829,123 & 513,153 & 2,490,028 & 733,816 & 341,718 & 753,631 \\
                    1000 & 236.26 & 370.09 & 1,475,528 & 659,686 & 4,063,488 & 1,121,781 & 1,827,617 & 438,549 \\
                    2000 & 344.52 & 633.09 & 1,534,024 & 648,545 & 4,825,887 & 1,109,934 & 2,605,998 & 289,271 \\
                    \hline
                \end{tabularx}
            \end{table}
            \begin{table}
                \centering
                \begin{tabularx}{\linewidth}{|X|X|X|X|X|X|X|X|X|X|X|}
                    \hline
                    \multicolumn{1}{|X|}{\centering Queue Size} & \multicolumn{1}{|X|}{\centering AP Received} & \multicolumn{1}{|X|}{\centering AP Unique} & \multicolumn{1}{|X|}{\centering Packets Sent} & \multicolumn{1}{|X|}{\centering Packets Successful} & \multicolumn{1}{|X|}{\centering Packets Stored} & \multicolumn{1}{|X|}{\centering Packets Dropped} & \multicolumn{1}{|X|}{\centering Queue Ignores Time} & \multicolumn{1}{|X|}{\centering Queue Ignores Space} & \multicolumn{1}{|X|}{\centering Queue Ignores Ratio} & \multicolumn{1}{|X|}{\centering \% Received} \\
                    \hline
                    10 & 463,908 & 421,623 & 2,160,545 & 463,239 & 57,673 & 833,967 & 8,183,045 & 8,567,042 & 0.955 & 33.73\% \\
                    30 & 626,482 & 499,921 & 2,200,064 & 598,384 & 128,859 & 759,144 & 7,126,313 & 7,348,423 & 0.975 & 39.99\% \\
                    100 & 829,123 & 513,153 & 2,490,028 & 733,816 & 341,718 & 753,631 & 8,189,153 & 8,429,989 & 0.971 & 41.05\% \\
                    1000 & 1,475,528 & 659,686 & 4,063,488 & 1,121,781 & 1,827,617 & 438,549 & 6,906,153 & 6,976,392 & 0.990 & 52.77\% \\
                    2000 & 1,534,025 & 648,545 & 4,825,887 & 1,109,934 & 2,605,998 & 289,271 & 5,808,998 & 5,644,930 & 1.030 & 51.88\% \\
                    \hline
                \end{tabularx}
            \end{table}
        \end{landscape}


    \section{Priority Queue}\label{data_gathering_performance_priority_queue}

        \plan[Priority Queue]{Explain how queue size changed things}
        %Only 125 buses

    \section{Conclusions}\label{data_gathering_performance_conclusions}

        \plan[Conclusions]{Conclude with summary}
        %Remember that this is just 145 buses
